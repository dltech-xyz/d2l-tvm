

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-06 10:42:09
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-06 10:48:08
 * @Description:
 * @TODO::
 * @Reference:
-->

＃ GPU架构



可以看出，一个SM被划分为4个处理块。 在每个块中，有16个算术单元（AU）用于处理float32数字，也称为FP32 CUDA内核。
一个SM总共有64个FP32 AU，它们能够每次执行64个float32运算符（例如FMA）。 除了寄存器文件和指令加载器/解码器外，SM还具有8个张量核。 每个张量核心每次都可以执行$4\times 4$的float16（或int8 / int4）矩阵乘积。 因此，我们可以将其称为FP16 AU，每个时钟计数为$2\times 4^3=128$运算符。 值得注意的是，在本章中，我们将不使用张量核心。 我们将在下一章中讨论如何利用它。

另一个区别是，SM仅具有L1缓存，这与CPU的L1缓存相似。 但是，我们可以将此存储用作SM上运行的所有线程的共享内存。 我们知道缓存受硬件和操作系统控制，同时我们可以显式分配和回收共享内存上的空间，这为我们提供了更大的灵活性来进行性能优化。

更广泛地说，我们在:numref:`tab_cpu_gpu_compare`中比较了本书中使用的CPU和GPU之间的规格差异，其中GPU包括[Tesla P100]（https://images.nvidia.com/content/pdf/tesla/whitepaper  /pascal-architecture-whitepaper.pdf）（在Colab中使用），[Tesla V100]（https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf）（在Amazon中配备）  EC2 P3实例）和[Tesla T4]（https://www.nvidia.com/content/dam/zh-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf  ）（在Amazon EC2 G4实例中配备）。

:比较常用的CPU和GPU，`x`表示不支持。  \ $^*$:Tesla P100使用FP32 CUDA内核处理FP16。

|硬件 | Intel E5-2686 v4 | Tesla P100 | Tesla V100 | Tesla T4 |
|------|------|------|------|------|
| 时钟频率 (GHz) | **3** | 1.48 | 1.53 | 1.59 |
| # cores | 16 | 56 | **80** | 40 |
| # FP64 AUs per core | 4 | **32** | **32** | x |
| # FP32 AUs per core | 8 | **64** | **64** | **64** |
| # FP16 AUs per core | x | x$^*$ | **8** | **8** |
| 缓存每核心 (KB) | **320** | 64 | 128 | 64 |
| 共享缓存 (MB)| **45** | 4 | 6 | 6 |
| 内存 (GB) | **240** | 16 | 16 | 16 |
| 最大内存带宽 (GB/sec) | 72 | 732 | **900** | 300 |
| FP64 TFLOPS | 0.38 | 4.7 | **7.8** | x |
| FP32 TFLOPS | 0.77 | 9.3 | **15.7** | 8.1 |
| FP16 TFLOPS | x | 18.7 | **125.3** | 65 |
:label:`tab_cpu_gpu_compare`

## 小结
GPU在概念上与CPU具有相似的架构，但是速度更快。
